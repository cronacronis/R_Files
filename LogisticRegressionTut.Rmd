---
title: "Model Update"
output: pdf_document
---
```{r,echo=FALSE,message=FALSE}
setwd("~/R_Files/")
```
### Need: A statistical model to predict an event

#### Given:

* The _Response Variable_ is binomial

* The _Explanatory (Predictor) Variables_ may be continuous or categorical

* Desire to use a _linear model_: Powerful in that we can express the effect of the predictors on the response through a linear predictor

#### Using linear regression on binomial Response Variable violates key assumptions

In linear regression there are several prerequisites. Two in particular:

* Error is normally distributed

* Response variable is normally distributed

When modeling "real-world" data, the above two requisites are relaxed. However, there are a few that are more strict:

* Response variable is continuous

* Allowance for negative values

* Variance of response variable is fairly constant

What happens when the response variable is binary? Error terms are either 0 or 1, variance is non-constant and negative values are nonsensical. Therefore, standard linear regression is inappropriate for binary response variables.

#### Some Definitions:

* Odds: $\frac{P}{1 - P}$

* Linear Regression: $E(Y) = \mu = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p = \sum_{i=0}^{p}\beta_i X_i$

#### Logistic Regression:

* Basic idea of Logistic Regression:
     + Use linear regression by modeling the probability ($P_i$) using a linear predictor function
     + Create a more "general" model while retaining the "linear approach"
     + Allows for non-linear relationships between predictors and response variable


* We introduce $\eta$:
     + $\eta = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ... + \beta_p X_p = \sum_{i=0}^{p}\beta_i X_i$
     + where $\eta = g(\mu)$
     + g() is the "link function" where $\eta$ links $\sum_{i=0}^{p}\beta_i X_i$ to $\mu$
  
  
* For a binomial Response Variable, g is the "logit":
     + $\eta = g(\mu) = log(\frac{\mu}{1 - \mu})$
     + NOTE: This is a natural log (base e)
  
  
* We now have our Logistic Regression Model (expressed in probabilities):
     + $log(\frac{P}{1 - P}) = log(odds) = \sum_{i=0}^{p}\beta_i X_i$
     
In Logistic Regression, the predicted mean ("fitted value") is a probability occuring [0,1].
  
  
#### How to Use and Interpret Logistic Regression Model:
* OK, but what about relating unit changes in $X_i$ to odds or calculating the probability of an event happening?

* Calculate odds of an event occuring: exponentiating the $\beta$'s for unit changes in $X$
     + $\beta_1$ can be interpreted with a unit increase in $X_1$ and all other variables fixed, as an increase in the odds of "success" (an event happening) by a factor of $e^{\beta_1}$

* Calculate the probability of success (an event occuring) by calculating the "fitted value" for a given X:
     + $log(\frac{P}{1 - P}) = -3.6210 + 0.850 X_1 = \xi$; where $\xi$ has just been calculated. In our case, $X_1$ may be number of cars
     + $P = \frac{1}{1 + e^{-\xi}}$
     
#### An example

In January 1986, Challenger exploded shortly after launch. Fact finding focused on O-ring seals in the rocket boosters, where it was found that at lower temperatures, rubber is a less effective sealant. At time of launch, temperature was 31 degrees (F). Could that have been predicted? 

Each shuttle had two boosters, with 3 O-rings each. In previous 23 missions, evidence of O-ring damage was recorded. For each mission, we know the number of O-rings out of 6 showing damage, and temperature.

We are interested in how O-ring failure is related to temperature, and the probability of failure at a given temperature. Oddly, we'll code a failure (extensive damage) = success (easier for modeling).

```{r,echo=FALSE,message=FALSE}
library(dplyr)
library(ggplot2)
library(MASS)
library(faraway)
data(orings)
lmod = lm(damage/6 ~ temp,orings)
```

The figure below demonstrates that Linear Regression is not a good model 

```{r,echo=FALSE,message=FALSE}
plot(damage/6 ~ temp,orings)
abline(lmod)
```

Create a logistic regression model using this data:
```{r,echo=FALSE,message=FALSE}
logitmod = glm(cbind(damage,6-damage) ~ temp, family=binomial(link=logit),orings)
```
```{r}
summary(logitmod)
```     
Use the model to calculate:

1. the effect of temperature on the odds of "success"
2. the probability of a "success" at 31 deg F


Using the coefficient `r coefficients(logitmod)[2]` for temperature we can determine the effect of temperature on the odds of "success":
```{r}
exp(coefficients(logitmod)[2])
```
This means that for each one degree increase in temp, odds of "success" are multipled by `r exp(coefficients(logitmod)[2])` (i.e., a reduction in odds). 

Using the logistic regression model we can determine the probability of "success" when the temp = 31 deg F
```{r}
1/(1 + exp(-(coefficients(logitmod)[1] + coefficients(logitmod)[2] * 31)))
```

Let's plot the model's estimate probabilities of "success" versus temperature

```{r,echo=FALSE}
plot(damage/6 ~ temp,orings,xlim=c(25,85), ylim=c(0,1),
     xlab="Temperature",ylab="Prob of damage")
x = seq(30,85,1)
#lines(x,ilogit(11.6630-0.2162*x))    # ilogit computes the inerse logit transformation
lines(x,1/(1 + exp(-(coefficients(logitmod)[1] + coefficients(logitmod)[2] * x))))
```

